{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie\n",
    "\n",
    "Przed rozpoczęciem pracy z notatnikiem proszę zmienić jego nazwę dodając na początku numer albumu, imię i nazwisko.\n",
    "{nr_albumu}\\_{imię}\\_{nazwisko}\\_{nazwa}\n",
    "\n",
    "Po wykonaniu wszystkich zadań proszę przesłać wypełniony notatnik przez platformę ELF za pomocą formularza \"Prześlij projekt\" w odpowiedniej sekcji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja liniowa wieloraka\n",
    "\n",
    "Rzadko kiedy zdarza się taka sytuacja, że zależność opisuje się na podstawie tylko jednej zmiennej. Z reguły na wynik zmiennej objaśnianej ($y$) ma wpły więcej różnych cech. Przykładowo, na cenę samochodu ma wpływ rok produkcji, przebieg, ilość koni mechanicznych itp. Dlatego właśnie jest naturalna potrzeba rozwinięcia algorytmu regresji liniowej z jedną cechą na większą ilość cech.\n",
    "\n",
    "Algorytm, który implementowaliśmy w poprzednim zadaniu jest szczególnym przypadkiem regresji liniowej, ale może zostać on w łatwy sposób uogólniony. Mechanizmy, które poznaliśmy wcześniej takie jak obliczanie funkcji błędu, pochodnych cząstkowych, w dalszym ciągu są aktualne. Trzeba jedynie uwzględnić dodatkowe cechy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "\n",
    "W zbiorze danych z zarobkami, który wykorzystywany był w poprzednim zadaniu, znajduje się pominięta wcześniej cecha. Wczytaj dane z pliku Salary.csv, tym razem z dwiema zmiennymi objaśniającymi: YearsExperience i Age oraz zmienną objaśnianą Salary. Stwórz wykres 3D przedstawiający dane."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Salary.csv', sep=',')\n",
    "\n",
    "x = df[\"YearsExperience\"]\n",
    "y = df[\"Age\"]\n",
    "z = df[\"Salary\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, c='blue', marker='o')\n",
    "ax.set_xlabel('YearsExperience')\n",
    "ax.set_ylabel('Age')\n",
    "ax.set_zlabel('Salary')\n",
    "ax.set_title('3D Scatter Plot')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2\n",
    "\n",
    "Przerób algorytm znajdujący się w funkcji _learn_and_fit(x,y)_ w taki sposób, aby uwzględniał dodatkową cechę.\n",
    "Funkcja regresji liniowej przybierze w tym momencie postać:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x^{(i)}) = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 = \\beta_{0} + \\beta_{1} YearsExperience + \\beta_{2} Age\n",
    "\\end{equation}\n",
    "\n",
    "Pojawienie się kolejnej cechy wymaga akutalizacji obliczania gradientu. Należy dodatkowo obliczyć pochodną cząstkową względem parametru $\\beta_{2}$, a następnie zaktualizować wartość tego parametru. \n",
    "\n",
    "Obliczenie pochodnej cząstkowej wygląda analogicznie jak w przypadku parametru $\\beta_{1}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial SSR}{\\partial \\beta_{2}} = \\frac{1}{n} \\sum^{n}_{i=1} (f(x^{(i)}) - y^{(i)})x_{1}^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Aktualizacja wartości współczynnika również jest analogiczna.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_{2} = \\beta_{2} - \\alpha \\frac{\\partial SSR}{\\partial \\beta_{2}} \n",
    "\\end{equation}\n",
    "\n",
    "_Uwaga: Zastanów się, w jaki sposób zaimplementować obługę kolejnych cech, tak aby po pojawieniu się 3 cechy nie trzeba było modyfikować algorytmu._"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "def initialize_coefficients(n: int = 2, alpha = None) -> Tuple[float, np.ndarray]:\n",
    "    if alpha is None:\n",
    "        alpha = random.random()\n",
    "\n",
    "    return alpha, np.array([random.random() for _ in range(n)])\n",
    "\n",
    "\n",
    "def calculate_regression_function(X: np.ndarray, betas: np.ndarray) -> np.ndarray:\n",
    "    return X @ betas\n",
    "\n",
    "\n",
    "def calculate_error(predictions: np.ndarray, y: np.ndarray, betas: np.ndarray) -> float:\n",
    "    m = y.shape[0]\n",
    "    return (np.sum((predictions - y)**2))/(2*m)\n",
    "\n",
    "\n",
    "def calculate_gradient(predictions: np.ndarray, X: np.ndarray, y: np.ndarray, betas: np.ndarray) -> np.ndarray:\n",
    "    m = y.shape[0]\n",
    "    diff = predictions - y\n",
    "    return (X.T @ diff)/m\n",
    "\n",
    "def update_regression_coefficients(X: np.ndarray, y: np.ndarray, betas: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    gradients = calculate_gradient(\n",
    "        calculate_regression_function(X,betas),\n",
    "        X,\n",
    "        y,\n",
    "        betas)\n",
    "    return betas - alpha * gradients"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "'''\n",
    "input:\n",
    "X - wartości zmiennych objaśniających YearsExperience oraz Age dla wszystkich obserwacji\n",
    "y - wartości zmiennej objaśnianej Salary dla wszystkich obserwacji\n",
    "\n",
    "output:\n",
    "b0: [] - lista z współczynnikami beta_0 w każdej z epok\n",
    "betas: [] - lista z współczynnikami beta_1, beta_2 w każdej z epok\n",
    "error: [] - lista z błędem w każdej epoce\n",
    "'''\n",
    "def learn_and_fit(X: np.ndarray, y: np.ndarray, alpha=0.1, epochs=100) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    X = (X - np.mean(X)) / np.std(X)\n",
    "    y = (y - np.mean(y)) / np.std(y)\n",
    "\n",
    "    # Add a column of ones for the bias (beta_0)\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "\n",
    "    errors = []\n",
    "    b0 = []\n",
    "    betas = []\n",
    "\n",
    "    alpha , betas_values = initialize_coefficients(n=X.shape[1] ,alpha=alpha)\n",
    "    tolerance = 1e-4\n",
    "\n",
    "    for i in range(epochs) :\n",
    "        predictions = calculate_regression_function(X, betas_values)\n",
    "        error = calculate_error(predictions, y, betas_values)\n",
    "        errors.append(error)\n",
    "        betas_values = update_regression_coefficients(X, y, betas_values, alpha)\n",
    "        b0.append(betas_values[0].copy())\n",
    "        betas.append(betas_values[1:].copy())\n",
    "\n",
    "        if i > 0 and abs(errors[-1] - errors[-2]) < tolerance:\n",
    "            print(f\"Stop at epoch {i}, error change < {tolerance}\")\n",
    "            break\n",
    "\n",
    "    return np.array(b0), np.array(betas), np.array(errors)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:13:54.355699Z",
     "start_time": "2025-04-18T09:13:54.330679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('Salary.csv', sep=',')\n",
    "\n",
    "xx = df[[\"YearsExperience\", \"Age\"]]\n",
    "y = df[\"Salary\"]\n",
    "print(np.array(xx))\n",
    "b0s, all_betas, errors = learn_and_fit(xx, y, alpha=0.01, epochs=200)\n",
    "print(f\"b0s: {b0s}\\n allbetas: {all_betas} \\n errors: {errors}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1 20. ]\n",
      " [ 1.3 21. ]\n",
      " [ 1.5 21. ]\n",
      " [ 2.  22. ]\n",
      " [ 2.2 22. ]\n",
      " [ 2.9 22. ]\n",
      " [ 3.  23. ]\n",
      " [ 3.2 24. ]\n",
      " [ 3.2 24. ]\n",
      " [ 3.7 24. ]\n",
      " [ 3.9 25. ]\n",
      " [ 4.  25. ]\n",
      " [ 4.  25. ]\n",
      " [ 4.1 25. ]\n",
      " [ 4.5 26. ]\n",
      " [ 4.9 26. ]\n",
      " [ 5.1 26. ]\n",
      " [ 5.3 27. ]\n",
      " [ 5.9 28. ]\n",
      " [ 6.  29. ]\n",
      " [ 6.8 29. ]\n",
      " [ 7.1 29. ]\n",
      " [ 7.9 31. ]\n",
      " [ 8.2 31. ]\n",
      " [ 8.7 32. ]\n",
      " [ 9.  32. ]\n",
      " [ 9.5 33. ]\n",
      " [ 9.6 34. ]\n",
      " [10.3 36. ]\n",
      " [10.5 34. ]\n",
      " [11.2 36. ]\n",
      " [11.5 36. ]\n",
      " [12.3 37. ]\n",
      " [12.9 40. ]\n",
      " [13.5 38. ]]\n",
      "b0s: [0.87632344 0.87287734 0.8695145  0.86623269 0.86302974 0.85990355\n",
      " 0.85685206 0.85387328 0.85096527 0.84812614 0.84535405 0.84264721\n",
      " 0.84000389 0.83742239 0.83490106 0.8324383  0.83003256 0.82768231\n",
      " 0.82538608 0.82314244 0.82094998 0.81880735 0.81671323 0.81466633\n",
      " 0.81266539 0.81070921 0.80879659 0.80692638 0.80509745 0.80330873\n",
      " 0.80155914 0.79984766 0.79817327 0.796535   0.79493189 0.79336303\n",
      " 0.7918275  0.79032444 0.78885299 0.78741232 0.78600162 0.78462011\n",
      " 0.78326701 0.7819416  0.78064314 0.77937093 0.77812428 0.77690254\n",
      " 0.77570504 0.77453116 0.77338028 0.7722518  0.77114516 0.77005977\n",
      " 0.76899508 0.76795057 0.76692571 0.76592    0.76493293 0.76396403\n",
      " 0.76301284 0.76207889 0.76116175 0.76026098 0.75937617 0.7585069\n",
      " 0.75765279 0.75681344 0.75598848 0.75517755 0.75438028 0.75359633\n",
      " 0.75282537 0.75206707 0.75132111 0.75058718 0.74986497 0.74915419\n",
      " 0.74845456 0.7477658  0.74708764 0.7464198  0.74576205 0.74511413\n",
      " 0.74447578 0.74384679 0.74322692 0.74261594 0.74201363 0.74141979\n",
      " 0.74083421 0.74025668 0.73968702 0.73912503 0.73857052 0.73802332\n",
      " 0.73748325 0.73695014 0.73642383 0.73590415 0.73539094 0.73488405\n",
      " 0.73438334 0.73388865 0.73339985 0.7329168  0.73243936 0.73196741\n",
      " 0.73150081 0.73103945 0.7305832  0.73013195 0.72968559 0.729244\n",
      " 0.72880707 0.7283747  0.72794679 0.72752324 0.72710394 0.72668882\n",
      " 0.72627777 0.72587071 0.72546754 0.72506819 0.72467257 0.7242806\n",
      " 0.7238922  0.7235073  0.72312583 0.7227477  0.72237285 0.72200122\n",
      " 0.72163273 0.72126732 0.72090493 0.72054549 0.72018895 0.71983524\n",
      " 0.71948432 0.71913611 0.71879059 0.71844767 0.71810733 0.7177695\n",
      " 0.71743415 0.71710121 0.71677065 0.71644242 0.71611648 0.71579278\n",
      " 0.71547128 0.71515195 0.71483474 0.71451961 0.71420654 0.71389547\n",
      " 0.71358638 0.71327923 0.71297398 0.71267061 0.71236909 0.71206937\n",
      " 0.71177144 0.71147525 0.71118079 0.71088803 0.71059693 0.71030747\n",
      " 0.71001962 0.70973336 0.70944867 0.70916551 0.70888388 0.70860373\n",
      " 0.70832506 0.70804783 0.70777203 0.70749764 0.70722463 0.70695299\n",
      " 0.7066827  0.70641374 0.70614608 0.70587972 0.70561464 0.70535081\n",
      " 0.70508822 0.70482685 0.7045667  0.70430774 0.70404996 0.70379334\n",
      " 0.70353787 0.70328353 0.70303032 0.70277821 0.7025272  0.70227727\n",
      " 0.70202841 0.70178061]\n",
      " allbetas: [[0.7648914  0.18827863]\n",
      " [0.77006358 0.18815916]\n",
      " [0.77515454 0.18810969]\n",
      " [0.78016633 0.18812812]\n",
      " [0.78510092 0.18821243]\n",
      " [0.78996022 0.18836062]\n",
      " [0.79474612 0.18857079]\n",
      " [0.79946043 0.18884106]\n",
      " [0.80410493 0.1891696 ]\n",
      " [0.80868133 0.18955466]\n",
      " [0.81319133 0.1899945 ]\n",
      " [0.81763654 0.19048747]\n",
      " [0.82201857 0.19103192]\n",
      " [0.82633895 0.19162627]\n",
      " [0.8305992  0.19226899]\n",
      " [0.83480077 0.19295858]\n",
      " [0.83894511 0.19369357]\n",
      " [0.84303358 0.19447256]\n",
      " [0.84706755 0.19529417]\n",
      " [0.85104832 0.19615706]\n",
      " [0.85497718 0.19705992]\n",
      " [0.85885538 0.19800148]\n",
      " [0.86268412 0.19898052]\n",
      " [0.86646458 0.19999584]\n",
      " [0.87019791 0.20104628]\n",
      " [0.87388523 0.20213069]\n",
      " [0.87752763 0.20324797]\n",
      " [0.88112617 0.20439707]\n",
      " [0.88468187 0.20557693]\n",
      " [0.88819573 0.20678654]\n",
      " [0.89166875 0.20802491]\n",
      " [0.89510185 0.2092911 ]\n",
      " [0.89849598 0.21058415]\n",
      " [0.90185202 0.21190319]\n",
      " [0.90517086 0.21324731]\n",
      " [0.90845335 0.21461566]\n",
      " [0.91170032 0.21600742]\n",
      " [0.91491258 0.21742177]\n",
      " [0.91809091 0.21885792]\n",
      " [0.92123608 0.22031511]\n",
      " [0.92434883 0.2217926 ]\n",
      " [0.9274299  0.22328967]\n",
      " [0.93047998 0.2248056 ]\n",
      " [0.93349977 0.22633972]\n",
      " [0.93648993 0.22789136]\n",
      " [0.93945112 0.22945987]\n",
      " [0.94238398 0.23104464]\n",
      " [0.94528911 0.23264504]\n",
      " [0.94816713 0.23426048]\n",
      " [0.95101861 0.23589039]\n",
      " [0.95384413 0.23753421]\n",
      " [0.95664425 0.23919138]\n",
      " [0.95941949 0.24086139]\n",
      " [0.9621704  0.24254371]\n",
      " [0.96489748 0.24423785]\n",
      " [0.96760123 0.24594332]\n",
      " [0.97028214 0.24765964]\n",
      " [0.97294069 0.24938636]\n",
      " [0.97557732 0.25112302]\n",
      " [0.97819249 0.25286921]\n",
      " [0.98078665 0.25462449]\n",
      " [0.9833602  0.25638845]\n",
      " [0.98591358 0.2581607 ]\n",
      " [0.98844718 0.25994084]\n",
      " [0.99096139 0.26172851]\n",
      " [0.9934566  0.26352334]\n",
      " [0.99593318 0.26532496]\n",
      " [0.9983915  0.26713305]\n",
      " [1.0008319  0.26894726]\n",
      " [1.00325474 0.27076726]\n",
      " [1.00566034 0.27259274]\n",
      " [1.00804904 0.2744234 ]\n",
      " [1.01042116 0.27625893]\n",
      " [1.012777   0.27809905]\n",
      " [1.01511686 0.27994346]\n",
      " [1.01744105 0.28179191]\n",
      " [1.01974984 0.28364412]\n",
      " [1.02204353 0.28549984]\n",
      " [1.02432237 0.28735881]\n",
      " [1.02658664 0.28922079]\n",
      " [1.02883659 0.29108555]\n",
      " [1.03107248 0.29295286]\n",
      " [1.03329456 0.29482249]\n",
      " [1.03550305 0.29669422]\n",
      " [1.0376982  0.29856785]\n",
      " [1.03988023 0.30044318]\n",
      " [1.04204936 0.30232   ]\n",
      " [1.04420581 0.30419812]\n",
      " [1.04634979 0.30607736]\n",
      " [1.04848151 0.30795754]\n",
      " [1.05060116 0.30983847]\n",
      " [1.05270894 0.31172   ]\n",
      " [1.05480503 0.31360195]\n",
      " [1.05688964 0.31548416]\n",
      " [1.05896293 0.31736649]\n",
      " [1.06102508 0.31924876]\n",
      " [1.06307626 0.32113086]\n",
      " [1.06511664 0.32301261]\n",
      " [1.06714639 0.3248939 ]\n",
      " [1.06916566 0.32677459]\n",
      " [1.07117461 0.32865455]\n",
      " [1.07317339 0.33053364]\n",
      " [1.07516214 0.33241176]\n",
      " [1.07714102 0.33428878]\n",
      " [1.07911016 0.33616459]\n",
      " [1.08106969 0.33803907]\n",
      " [1.08301976 0.33991212]\n",
      " [1.0849605  0.34178364]\n",
      " [1.08689202 0.34365352]\n",
      " [1.08881446 0.34552166]\n",
      " [1.09072793 0.34738798]\n",
      " [1.09263256 0.34925238]\n",
      " [1.09452846 0.35111476]\n",
      " [1.09641574 0.35297505]\n",
      " [1.09829451 0.35483317]\n",
      " [1.10016488 0.35668902]\n",
      " [1.10202695 0.35854253]\n",
      " [1.10388084 0.36039363]\n",
      " [1.10572662 0.36224225]\n",
      " [1.10756442 0.3640883 ]\n",
      " [1.10939431 0.36593174]\n",
      " [1.11121639 0.36777248]\n",
      " [1.11303076 0.36961046]\n",
      " [1.1148375  0.37144563]\n",
      " [1.1166367  0.37327791]\n",
      " [1.11842844 0.37510727]\n",
      " [1.12021281 0.37693363]\n",
      " [1.12198988 0.37875695]\n",
      " [1.12375974 0.38057717]\n",
      " [1.12552247 0.38239424]\n",
      " [1.12727813 0.38420812]\n",
      " [1.1290268  0.38601876]\n",
      " [1.13076856 0.3878261 ]\n",
      " [1.13250347 0.38963012]\n",
      " [1.13423161 0.39143077]\n",
      " [1.13595304 0.393228  ]\n",
      " [1.13766782 0.39502177]\n",
      " [1.13937603 0.39681206]\n",
      " [1.14107772 0.39859882]\n",
      " [1.14277296 0.40038202]\n",
      " [1.1444618  0.40216163]\n",
      " [1.14614431 0.40393761]\n",
      " [1.14782054 0.40570993]\n",
      " [1.14949055 0.40747856]\n",
      " [1.1511544  0.40924348]\n",
      " [1.15281214 0.41100466]\n",
      " [1.15446381 0.41276206]\n",
      " [1.15610948 0.41451568]\n",
      " [1.1577492  0.41626547]\n",
      " [1.15938301 0.41801142]\n",
      " [1.16101096 0.41975351]\n",
      " [1.16263311 0.42149171]\n",
      " [1.16424949 0.423226  ]\n",
      " [1.16586015 0.42495637]\n",
      " [1.16746515 0.4266828 ]\n",
      " [1.16906451 0.42840527]\n",
      " [1.17065829 0.43012376]\n",
      " [1.17224653 0.43183826]\n",
      " [1.17382927 0.43354874]\n",
      " [1.17540654 0.43525521]\n",
      " [1.1769784  0.43695764]\n",
      " [1.17854487 0.43865602]\n",
      " [1.18010601 0.44035033]\n",
      " [1.18166183 0.44204058]\n",
      " [1.18321239 0.44372674]\n",
      " [1.18475771 0.4454088 ]\n",
      " [1.18629784 0.44708677]\n",
      " [1.18783281 0.44876061]\n",
      " [1.18936265 0.45043034]\n",
      " [1.19088739 0.45209594]\n",
      " [1.19240707 0.45375741]\n",
      " [1.19392172 0.45541473]\n",
      " [1.19543138 0.4570679 ]\n",
      " [1.19693607 0.45871692]\n",
      " [1.19843583 0.46036177]\n",
      " [1.19993068 0.46200247]\n",
      " [1.20142065 0.46363899]\n",
      " [1.20290578 0.46527134]\n",
      " [1.20438609 0.46689952]\n",
      " [1.20586162 0.46852351]\n",
      " [1.20733238 0.47014333]\n",
      " [1.2087984  0.47175896]\n",
      " [1.21025972 0.47337041]\n",
      " [1.21171636 0.47497767]\n",
      " [1.21316835 0.47658074]\n",
      " [1.2146157  0.47817963]\n",
      " [1.21605845 0.47977433]\n",
      " [1.21749662 0.48136483]\n",
      " [1.21893024 0.48295116]\n",
      " [1.22035932 0.48453329]\n",
      " [1.2217839  0.48611124]\n",
      " [1.22320399 0.48768501]\n",
      " [1.22461962 0.48925459]\n",
      " [1.22603082 0.49081999]\n",
      " [1.2274376  0.49238121]\n",
      " [1.22883998 0.49393826]\n",
      " [1.23023799 0.49549113]\n",
      " [1.23163165 0.49703983]\n",
      " [1.23302098 0.49858436]\n",
      " [1.23440599 0.50012473]] \n",
      " errors: [0.30391354 0.29993831 0.29610995 0.29242099 0.28886435 0.28543334\n",
      " 0.2821216  0.27892314 0.27583226 0.27284358 0.26995199 0.26715268\n",
      " 0.26444106 0.2618128  0.25926381 0.25679019 0.25438828 0.25205458\n",
      " 0.2497858  0.24757881 0.24543066 0.24333853 0.24129977 0.23931187\n",
      " 0.23737244 0.23547923 0.2336301  0.23182301 0.23005606 0.22832741\n",
      " 0.22663535 0.22497825 0.22335455 0.22176279 0.22020158 0.2186696\n",
      " 0.2171656  0.21568839 0.21423686 0.21280993 0.2114066  0.21002592\n",
      " 0.20866697 0.20732889 0.20601087 0.20471213 0.20343194 0.20216959\n",
      " 0.20092443 0.19969583 0.19848319 0.19728595 0.19610357 0.19493554\n",
      " 0.19378137 0.1926406  0.1915128  0.19039755 0.18929445 0.18820314\n",
      " 0.18712324 0.18605443 0.18499638 0.18394878 0.18291135 0.18188379\n",
      " 0.18086586 0.17985728 0.17885784 0.17786728 0.17688541 0.17591201\n",
      " 0.17494687 0.17398982 0.17304068 0.17209926 0.1711654  0.17023895\n",
      " 0.16931976 0.16840768 0.16750258 0.16660432 0.16571279 0.16482785\n",
      " 0.16394939 0.1630773  0.16221148 0.16135183 0.16049824 0.15965063\n",
      " 0.15880889 0.15797296 0.15714274 0.15631815 0.15549911 0.15468556\n",
      " 0.15387742 0.15307461 0.15227708 0.15148477 0.15069759 0.14991551\n",
      " 0.14913845 0.14836637 0.14759921 0.14683691 0.14607943 0.14532671\n",
      " 0.14457871 0.14383537 0.14309666 0.14236254 0.14163294 0.14090785\n",
      " 0.14018721 0.13947098 0.13875912 0.13805161 0.13734839 0.13664944\n",
      " 0.13595472 0.1352642  0.13457783 0.13389559 0.13321745 0.13254337\n",
      " 0.13187333 0.13120728 0.13054521 0.12988707 0.12923286 0.12858252\n",
      " 0.12793605 0.1272934  0.12665455 0.12601948 0.12538816 0.12476057\n",
      " 0.12413667 0.12351644 0.12289986 0.12228691 0.12167755 0.12107177\n",
      " 0.12046954 0.11987084 0.11927565 0.11868394 0.11809569 0.11751088\n",
      " 0.11692948 0.11635148 0.11577686 0.11520558 0.11463764 0.11407301\n",
      " 0.11351167 0.11295359 0.11239877 0.11184717 0.11129879 0.11075359\n",
      " 0.11021156 0.10967269 0.10913694 0.10860431 0.10807478 0.10754831\n",
      " 0.10702491 0.10650454 0.10598719 0.10547284 0.10496148 0.10445309\n",
      " 0.10394764 0.10344512 0.10294552 0.10244881 0.10195498 0.10146401\n",
      " 0.10097589 0.10049059 0.10000811 0.09952842 0.09905151 0.09857736\n",
      " 0.09810596 0.09763728 0.09717132 0.09670806 0.09624748 0.09578956\n",
      " 0.0953343  0.09488167 0.09443166 0.09398425 0.09353944 0.09309719\n",
      " 0.09265751 0.09222037]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "\n",
    "Do stworzonego z zadaniu 1 wykresu dodaj płaszczyznę regresji. Stwórz 3 wykresy przedstawiające jak zmieniała się funkcja regresji na przestrzeni epok (pierwsza, środkowa, ostatnia epoka)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Normalize input data (if not already normalized)\n",
    "xx = (xx - np.mean(xx, axis=0)) / np.std(xx, axis=0)\n",
    "z = (y - np.mean(y, axis=0)) / np.std(y, axis=0)\n",
    "\n",
    "# Generate surface grid\n",
    "x_surf, y_surf = np.meshgrid(\n",
    "    np.linspace(xx.iloc[:, 0].min(), xx.iloc[:, 0].max(), 100),\n",
    "    np.linspace(xx.iloc[:, 1].min(), xx.iloc[:, 1].max(), 100)\n",
    ")\n",
    "\n",
    "# Calculate predicted z values (regression plane)\n",
    "z_surf = b0s[-1] + all_betas[-1][0] * x_surf + all_betas[-1][1] * y_surf\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add regression surface\n",
    "fig.add_trace(go.Surface(\n",
    "    x=x_surf,\n",
    "    y=y_surf,\n",
    "    z=z_surf,\n",
    "    colorscale='Blues',\n",
    "    opacity=0.6,\n",
    "    name='Regression Plane',\n",
    "    showscale=False\n",
    "))\n",
    "\n",
    "# Add scatter plot (data points)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=xx.iloc[:, 0],\n",
    "    y=xx.iloc[:, 1],\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='red'),\n",
    "    name='Data Points'\n",
    "))\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    title='3D Regression Plane with Data Points (Plotly)',\n",
    "    scene=dict(\n",
    "        xaxis_title='YearsExperience',\n",
    "        yaxis_title='Age',\n",
    "        zaxis_title='Salary'\n",
    "    ),\n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4\n",
    "\n",
    "W sytuacji, w której zbiór danych zawiera więcej zmiennych objaśniających niż 2, niemożliwym staje się wizualizacja prostej regresji i ocena w taki sposób stworzonego modelu. Bardzo przydatnym rozwiązaniem jest wtedy stworzenie wykresu błędów regresji. Jeśli wartości błędu spadają wraz z kolejnymi epokami, oznacza to, że jesteśmy na dobrej drodze, a nasz algorytm działa poprawnie. Celem tego zadania będzie stworzenie finalnego modelu regresji liniowej, który będzie przyjmował dowolną liczbę zmiennych objaśniających.\n",
    "\n",
    "Na podstawie wcześniejszych implementacji, stwórz implementację funkcji *learn_and_fit_multi(X, y)*, która będzie przyjmować zbiór wejściowy z dowolną ilością kolum (cech). Dla takiego zbioru zbioru danych ma zostać stworzony model regresji. Funkcja podobnie jak wcześniej, ma zwracać współczynniki oraz wartość błędu w każdej epoce. \n",
    "\n",
    "W notebooku z opisem regresji liniowej przedstawione zostały wzory na ogólą postać regresji. Przeanalizuj je jeszcze raz i postaraj się je zaimplementować.\n",
    "\n",
    "Wczytaj zestaw danych *multi_variable_regression.csv* z katalogu datasets. Dane wygenerowane zostały w taki sposób, że są wysoce liniowo zależne. Wartość błędu dla nauczonego modelu powinna być w takim przypadku niewielka. Przetestuj na wczytanym zbiorze swój algorytm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"multi_variable_regression.csv\")\n",
    "\n",
    "# Algorithm for multi variable regression has been implemented in task 2.\n",
    "xx = df.iloc[:, :-2]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "b0s, all_betas, errors = learn_and_fit(xx, y, alpha=0.01, epochs=200)\n",
    "print(f\"errors: {errors}\")\n",
    "print(f\"{errors.shape}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 5\n",
    "\n",
    "Stwórz wykres przedstawiający zmianę błędu regresji w kolejnych epokach. Napisz co można na jego podstawie wywnioskować."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(errors)), errors, marker='o', linestyle='-', markersize=2)\n",
    "plt.title(\"Regression Error chart\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 6\n",
    "\n",
    "W jaki sposób współczynnik alpha wpływa na działania algorytmu? Przeprowadź eksperyment dla minimum trzech różnych wartości tego parametru. Sformułuj wnioski. Jak zmiana parametru wpłynęła na ilość epok w algorytmie? Jak zmieniła się funkcja regresji?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"multi_variable_regression.csv\")\n",
    "\n",
    "# Algorithm for multi variable regression has been implemented in task 2.\n",
    "xx = df.iloc[:, :-2]\n",
    "y = df.iloc[:, -1]\n",
    "alpha_list = [0.001, 0.01, 0.1, 0.5, 1]\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    b0s, all_betas, errors = learn_and_fit(xx, y, alpha=alpha, epochs=300)\n",
    "    print(f\"alpha: {alpha}\")\n",
    "    print(f\"error: {errors[-1]}\")\n",
    "    print(f\"epochs: {errors.shape[0]}\")\n",
    "    plt.plot(errors, label=f'alpha={alpha}')\n",
    "plt.legend()\n",
    "plt.title(\"The influence of alpha coefficient on errors in epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# As we can observe, alpha >= 0.01 gives similar effect regarding error value, but higher alpha => less number of epochs to train."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównaj czas działania algorytmu we własnej implementacji oraz implementacji z biblioteki Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"multi_variable_regression.csv\")\n",
    "xx = df.iloc[:, :-2]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    b0, betas, errors = learn_and_fit(xx, y, alpha=0.01, epochs=200)\n",
    "my_algorithm_time = time.time() - start_time\n",
    "\n",
    "# scikit-learn\n",
    "xx = (xx - np.mean(xx, axis=0)) / np.std(xx, axis=0)\n",
    "y = (y - np.mean(y, axis=0)) / np.std(y, axis=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    model.fit(xx, y)\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "plt.plot(errors)\n",
    "plt.title(\"Error per epoch - my implementation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"My algorithm - time: {my_algorithm_time:.4f} sec\")\n",
    "print(f\"Scikit-learn - time: {sklearn_time:.4f} sec\")\n",
    "\n",
    "# MSE error\n",
    "y_pred = model.predict(xx)\n",
    "error_sklearn = np.mean((y_pred - y) ** 2)\n",
    "print(f\"Error for scikit-learn: {error_sklearn:.4f}\")\n",
    "print(f\"Error for my implementation: {errors[-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
